{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8c23f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "# electronics = pd.read_csv('amazonRatings.csv', names=['index','userId','productId','Rating','timestamp'])\n",
    "# electronics = electronics.drop([0])\n",
    "# electronics = electronics.drop(['index'], axis=1)\n",
    "# electronics.to_csv('cleanedElec.csv',index=False,header=False)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "class DataProcess(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._loadData()\n",
    "        self._preProcess()\n",
    "        self._binarize(self._originalRatings)\n",
    "        # 对'userId'这一列的数据，先去重，然后构成一个用户列表\n",
    "        self._userPool = set(self._originalRatings['userId'].unique())\n",
    "        self._itemPool = set(self._originalRatings['itemId'].unique())\n",
    "        print(\"user_pool size: \", len(self._userPool))\n",
    "        print(\"item_pool size: \", len(self._itemPool))\n",
    "\n",
    "        self._select_Negatives(self._originalRatings)\n",
    "        self._split_pool(self._preprocessRatings)\n",
    "\n",
    "    def _loadData(self):\n",
    "        self._originalRatings = pd.read_csv(self._filename,names=['uid','mid','rating','timestamp','sust'])\n",
    "        is_multi_item = self._originalRatings['mid'].value_counts()>2\n",
    "        self._originalRatings = self._originalRatings[self._originalRatings['mid'].isin(is_multi_item[is_multi_item].index)]\n",
    "        is_multi_user=self._originalRatings['uid'].value_counts()>6\n",
    "        self._originalRatings = self._originalRatings[self._originalRatings['uid'].isin(is_multi_user[is_multi_user].index)]\n",
    "        return self._originalRatings\n",
    "\n",
    "    def _preProcess(self):\n",
    "        \"\"\"\n",
    "        对user和item都重新编号，这里这么做的原因是因为，模型的输入是one-hot向量，需要把user和item都限制在Embedding的长度之内，\n",
    "        模型的两个输入的长度分别是user和item的数量，所以要重新从0编号。\n",
    "        \"\"\"\n",
    "        # 1. 新建名为\"userId\"的列，这列对用户从0开始编号\n",
    "        user_id = self._originalRatings[['uid']].drop_duplicates().reindex()\n",
    "        user_id['userId'] = np.arange(len(user_id)) #根据user的长度创建一个数组\n",
    "        # 将原先的DataFrame与user_id按照\"uid\"这一列进行合并\n",
    "        self._originalRatings = pd.merge(self._originalRatings, user_id, on=['uid'], how='left')\n",
    "\n",
    "        # 2. 对物品进行重新排列\n",
    "        item_id = self._originalRatings[['mid']].drop_duplicates()\n",
    "        item_id['itemId'] = np.arange(len(item_id))\n",
    "        self._originalRatings = pd.merge(self._originalRatings, item_id, on=['mid'], how='left')\n",
    "\n",
    "        # 按照['userId', 'itemId', 'rating', 'timestamp']的顺序重新排列\n",
    "        self._originalRatings = self._originalRatings[['userId', 'itemId', 'rating', 'timestamp','sust']]\n",
    "        print(self._originalRatings)\n",
    "        print('Range of userId is [{}, {}]'.format(self._originalRatings.userId.min(), self._originalRatings.userId.max()))\n",
    "        print('Range of itemId is [{}, {}]'.format(self._originalRatings.itemId.min(), self._originalRatings.itemId.max()))\n",
    "\n",
    "    def _binarize(self, ratings):\n",
    "        \"\"\"\n",
    "        binarize data into 0 or 1 for implicit feedback\n",
    "        \"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
    "        self._preprocessRatings = ratings\n",
    "        # print(\"binary: \\n\", self._preprocessRatings)\n",
    "\n",
    "    def _select_Negatives(self, ratings):\n",
    "        \"\"\"\n",
    "        Select al;l negative samples and 100 sampled negative items for each user.\n",
    "        \"\"\"\n",
    "        # 构造user-item表\n",
    "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
    "            columns={'itemId': 'interacted_items'})\n",
    "        print(\"interact_status: \\n\", interact_status)\n",
    "\n",
    "        # 把与用户没有产生过交互的样本都当做是负样本\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self._itemPool - x)\n",
    "\n",
    "        # 从上面的全部负样本中随机选99个出来\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
    "        print(\"after sampling interact_status: \\n\", interact_status)\n",
    "\n",
    "        print(\"select and rearrange columns\")\n",
    "        self._negatives = interact_status[['userId', 'negative_items', 'negative_samples']]\n",
    "\n",
    "    def _split_pool(self, ratings):\n",
    "        \"\"\"leave one out train/test split \"\"\"\n",
    "        print(\"sort by timestamp descend\")\n",
    "        # 先按照'userID'进行分组，然后根据时间戳降序排列\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        print(ratings)\n",
    "\n",
    "        # 选取排名第一的数据作为测试集，也就是最新的那个数据\n",
    "        test = ratings[ratings['rank_latest'] == 1]\n",
    "        # 选取所有排名靠后的，也就是历史数据当做训练集\n",
    "        train = ratings[ratings['rank_latest'] > 1]\n",
    "        # print(\"test: \\n\", test)\n",
    "        # print(\"train: \\n\", train)\n",
    "\n",
    "        print(\"size of test {0}, size of train {1}\".format(len(test), len(train)))\n",
    "\n",
    "        # 确保训练集和测试集的userId是一样的\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "\n",
    "        self.train_ratings = train[['userId', 'itemId', 'rating']]\n",
    "        self.test_ratings = test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def sample_generator(self, num_negatives):\n",
    "        # 合并之后的train_ratings的列包括['userId','itemId'，'rating','negative_items']\n",
    "        train_ratings = pd.merge(self.train_ratings, self._negatives[['userId', 'negative_items']], on='userId')\n",
    "        # 从用户的全部负样本集合中随机选择num_negatives个样本当做负样本，并产生一个新的名为\"negatives\"的列\n",
    "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "        print(train_ratings)\n",
    "\n",
    "        # 构造模型所需要的数据，分别是输入user、items以及目标分值ratings。\n",
    "        users, items, ratings = [], [], []\n",
    "        for row in train_ratings.itertuples():\n",
    "            # 构造正样本，分别是userId， itemId以及目标分值1\n",
    "            users.append(int(row.userId))\n",
    "            items.append(int(row.itemId))\n",
    "            ratings.append(float(row.rating))\n",
    "            # 为每个用户构造num_negatives个负样本，分别是userId， itemId以及目标分值0\n",
    "            for i in range(num_negatives):\n",
    "                users.append(int(row.userId))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0)) # 负样本的ratings为0，直接强行设置为0\n",
    "\n",
    "        return users, items, ratings\n",
    "    \n",
    "    def test_neg_generator(self, num_negatives):\n",
    "        # 合并之后的test_ratings的列包括['userId','itemId'，'rating','negative_items']\n",
    "        test_ratings = pd.merge(self.test_ratings, self._negatives[['userId', 'negative_items']], on='userId')\n",
    "        # 从用户的全部负样本集合中随机选择num_negatives个样本当做负样本，并产生一个新的名为\"negatives\"的列\n",
    "        test_ratings['negatives'] = test_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "        print(test_ratings)\n",
    "\n",
    "        # 构造模型所需要的数据，分别是输入user、items以及目标分值ratings。\n",
    "        users, items, ratings = [], [], []\n",
    "        for row in test_ratings.itertuples():\n",
    "            # 构造正样本，分别是userId， itemId以及目标分值1\n",
    "            users.append(int(row.userId))\n",
    "            items.append(int(row.itemId))\n",
    "            ratings.append(float(row.rating))\n",
    "            # 为每个用户构造num_negatives个负样本，分别是userId， itemId以及目标分值0\n",
    "            for i in range(num_negatives):\n",
    "                users.append(int(row.userId))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0)) # 负样本的ratings为0，直接强行设置为0\n",
    "\n",
    "        return test_ratings\n",
    "\n",
    "mag = DataProcess('duplicated_sample.csv')\n",
    "\n",
    "# generating files for test and train ratings\n",
    "train_ratings = mag.train_ratings\n",
    "train_ratings.to_csv('ml-1m.train.rating.csv',header=None, index=None)\n",
    "test_ratings = mag.test_ratings\n",
    "test_ratings.to_csv('ml-1m.test.rating.csv',header=None, index=None)\n",
    "\n",
    "# generating file for test negative\n",
    "ndf = mag.test_neg_generator(99)\n",
    "ndf['uipair'] = list(zip(ndf.userId, ndf.itemId))\n",
    "\n",
    "ndf2 = ndf.loc[:, ['uipair','negatives']]\n",
    "ndf2.to_csv('ml-1m.test.negative.csv',sep='\\t', header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating file for sustainable items\n",
    "susRatings = mag._originalRatings[mag._originalRatings['sust']==1]\n",
    "susRatings['itemId'].to_csv('sust.csv',header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35285a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(susRatings['itemId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4cf55",
   "metadata": {},
   "source": [
    "### (trial) filter user based on duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "sampled1 = pd.read_csv('sampledData.csv',names=['uid','mid','rating','timestamp','sust'])\n",
    "sampled1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18495536",
   "metadata": {},
   "outputs": [],
   "source": [
    "noLabel2 = sampled1[sampled1['sust']==0]\n",
    "is_multi = noLabel2['uid'].value_counts()>5\n",
    "noLabel3 = noLabel2[noLabel2['uid'].isin(is_multi[is_multi].index)]\n",
    "noLabel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(noLabel3['uid'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ff5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "withLabel = sampled1[sampled1['sust']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([noLabel3,withLabel])\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf87554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(finalDf['uid'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmazonRecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aec2b49f0b156ccbd13eb3ced1f831c079eaadace1c55dfffcf3bc16a9a14954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
