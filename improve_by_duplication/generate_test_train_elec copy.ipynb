{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc8c23f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  itemId  rating   timestamp  sust\n",
      "0            0       0     2.0  1429401600     1\n",
      "1            1       0     5.0  1425427200     1\n",
      "2            2       0     5.0  1404604800     1\n",
      "3            3       0     5.0  1403049600     1\n",
      "4            4       0     5.0  1400112000     1\n",
      "...        ...     ...     ...         ...   ...\n",
      "245454   14812   12162     5.0  1470355200     1\n",
      "245455   15826   12372     4.0  1515456000     1\n",
      "245456   18198   12419     5.0  1516147200     1\n",
      "245457   19770   12931     5.0  1534204800     1\n",
      "245458    4278   12931     3.0  1534204800     1\n",
      "\n",
      "[245459 rows x 5 columns]\n",
      "Range of userId is [0, 21540]\n",
      "Range of itemId is [0, 19934]\n",
      "user_pool size:  21541\n",
      "item_pool size:  19935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuho\\AppData\\Local\\Temp\\ipykernel_14400\\2321854066.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['rating'][ratings['rating'] > 0] = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interact_status: \n",
      "        userId                                   interacted_items\n",
      "0           0                   {0, 6432, 1986, 421, 1324, 4280}\n",
      "1           1  {0, 7969, 5505, 5507, 5575, 7679, 8857, 7061, ...\n",
      "2           2            {0, 64, 12291, 6564, 2279, 12553, 4402}\n",
      "3           3              {0, 1410, 715, 2667, 3291, 1652, 603}\n",
      "4           4  {0, 9923, 4455, 10536, 8236, 6701, 6989, 4435,...\n",
      "...       ...                                                ...\n",
      "21536   21536   {10947, 10758, 12044, 12237, 8881, 10972, 11709}\n",
      "21537   21537  {12641, 9890, 9283, 10627, 10056, 12236, 12783...\n",
      "21538   21538       {18592, 9283, 10054, 9483, 9487, 9495, 9979}\n",
      "21539   21539     {9314, 9315, 18501, 18502, 18504, 9887, 18495}\n",
      "21540   21540  {18913, 11811, 10091, 19506, 19507, 19508, 10109}\n",
      "\n",
      "[21541 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuho\\AppData\\Local\\Temp\\ipykernel_14400\\2321854066.py:83: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after sampling interact_status: \n",
      "        userId                                   interacted_items  \\\n",
      "0           0                   {0, 6432, 1986, 421, 1324, 4280}   \n",
      "1           1  {0, 7969, 5505, 5507, 5575, 7679, 8857, 7061, ...   \n",
      "2           2            {0, 64, 12291, 6564, 2279, 12553, 4402}   \n",
      "3           3              {0, 1410, 715, 2667, 3291, 1652, 603}   \n",
      "4           4  {0, 9923, 4455, 10536, 8236, 6701, 6989, 4435,...   \n",
      "...       ...                                                ...   \n",
      "21536   21536   {10947, 10758, 12044, 12237, 8881, 10972, 11709}   \n",
      "21537   21537  {12641, 9890, 9283, 10627, 10056, 12236, 12783...   \n",
      "21538   21538       {18592, 9283, 10054, 9483, 9487, 9495, 9979}   \n",
      "21539   21539     {9314, 9315, 18501, 18502, 18504, 9887, 18495}   \n",
      "21540   21540  {18913, 11811, 10091, 19506, 19507, 19508, 10109}   \n",
      "\n",
      "                                          negative_items  \\\n",
      "0      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "1      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "2      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "3      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "4      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "...                                                  ...   \n",
      "21536  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21537  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21538  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21539  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21540  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "\n",
      "                                        negative_samples  \n",
      "0      [12629, 13787, 1329, 8490, 16759, 15928, 13274...  \n",
      "1      [7058, 19337, 13754, 19006, 9029, 14775, 16154...  \n",
      "2      [409, 15013, 2594, 11012, 1499, 17843, 9208, 4...  \n",
      "3      [14275, 12211, 17633, 5849, 6817, 12313, 19245...  \n",
      "4      [7081, 4799, 3430, 6492, 15032, 12398, 11861, ...  \n",
      "...                                                  ...  \n",
      "21536  [10713, 2417, 18735, 15223, 5778, 16250, 19769...  \n",
      "21537  [1086, 2458, 14096, 19031, 12224, 1225, 12884,...  \n",
      "21538  [15153, 17492, 424, 3562, 680, 4313, 14571, 40...  \n",
      "21539  [12589, 4790, 6261, 15846, 8761, 13185, 12695,...  \n",
      "21540  [8253, 19873, 10567, 14902, 9920, 1170, 13667,...  \n",
      "\n",
      "[21541 rows x 4 columns]\n",
      "select and rearrange columns\n",
      "sort by timestamp descend\n",
      "        userId  itemId  rating   timestamp  sust  rank_latest\n",
      "0            0       0     1.0  1429401600     1          1.0\n",
      "1            1       0     1.0  1425427200     1          1.0\n",
      "2            2       0     1.0  1404604800     1          4.0\n",
      "3            3       0     1.0  1403049600     1          1.0\n",
      "4            4       0     1.0  1400112000     1         11.0\n",
      "...        ...     ...     ...         ...   ...          ...\n",
      "245454   14812   12162     1.0  1470355200     1          2.0\n",
      "245455   15826   12372     1.0  1515456000     1          7.0\n",
      "245456   18198   12419     1.0  1516147200     1          6.0\n",
      "245457   19770   12931     1.0  1534204800     1          2.0\n",
      "245458    4278   12931     1.0  1534204800     1          2.0\n",
      "\n",
      "[245459 rows x 6 columns]\n",
      "size of test 21541, size of train 223918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuho\\AppData\\Local\\Temp\\ipykernel_14400\\2321854066.py:137: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  test_ratings['negatives'] = test_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       userId  itemId  rating  \\\n",
      "0           0       0     1.0   \n",
      "1           1       0     1.0   \n",
      "2           3       0     1.0   \n",
      "3           5       1     1.0   \n",
      "4           7       1     1.0   \n",
      "...       ...     ...     ...   \n",
      "21536   15961   19929     1.0   \n",
      "21537   14871   19930     1.0   \n",
      "21538   10033   19933     1.0   \n",
      "21539   11902   12955     1.0   \n",
      "21540   12616   12959     1.0   \n",
      "\n",
      "                                          negative_items  \\\n",
      "0      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "1      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "2      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "3      {0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "4      {0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "...                                                  ...   \n",
      "21536  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21537  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21538  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21539  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "21540  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "\n",
      "                                               negatives  \n",
      "0      [17515, 4501, 17571, 8402, 10982, 16614, 8579,...  \n",
      "1      [12809, 894, 11010, 4446, 15269, 17821, 9202, ...  \n",
      "2      [13261, 14599, 12254, 18292, 8991, 17441, 9127...  \n",
      "3      [346, 5645, 6639, 11121, 17348, 3933, 11435, 8...  \n",
      "4      [16685, 4357, 1725, 3413, 8113, 1847, 1048, 19...  \n",
      "...                                                  ...  \n",
      "21536  [7186, 4970, 1873, 780, 17267, 8613, 11551, 70...  \n",
      "21537  [10987, 6875, 5291, 2170, 13590, 2917, 4641, 4...  \n",
      "21538  [960, 6569, 16508, 4934, 12685, 16243, 6332, 9...  \n",
      "21539  [13019, 430, 2215, 9640, 16260, 8214, 18617, 9...  \n",
      "21540  [10720, 13993, 19263, 14129, 10917, 3558, 1326...  \n",
      "\n",
      "[21541 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "# electronics = pd.read_csv('amazonRatings.csv', names=['index','userId','productId','Rating','timestamp'])\n",
    "# electronics = electronics.drop([0])\n",
    "# electronics = electronics.drop(['index'], axis=1)\n",
    "# electronics.to_csv('cleanedElec.csv',index=False,header=False)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "class DataProcess(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._loadData()\n",
    "        self._preProcess()\n",
    "        self._binarize(self._originalRatings)\n",
    "        # 对'userId'这一列的数据，先去重，然后构成一个用户列表\n",
    "        self._userPool = set(self._originalRatings['userId'].unique())\n",
    "        self._itemPool = set(self._originalRatings['itemId'].unique())\n",
    "        print(\"user_pool size: \", len(self._userPool))\n",
    "        print(\"item_pool size: \", len(self._itemPool))\n",
    "\n",
    "        self._select_Negatives(self._originalRatings)\n",
    "        self._split_pool(self._preprocessRatings)\n",
    "\n",
    "    def _loadData(self):\n",
    "        self._originalRatings = pd.read_csv(self._filename,names=['uid','mid','rating','timestamp','sust'])\n",
    "        is_multi_item = self._originalRatings['mid'].value_counts()>2\n",
    "        self._originalRatings = self._originalRatings[self._originalRatings['mid'].isin(is_multi_item[is_multi_item].index)]\n",
    "        is_multi_user=self._originalRatings['uid'].value_counts()>6\n",
    "        self._originalRatings = self._originalRatings[self._originalRatings['uid'].isin(is_multi_user[is_multi_user].index)]\n",
    "        return self._originalRatings\n",
    "\n",
    "    def _preProcess(self):\n",
    "        \"\"\"\n",
    "        对user和item都重新编号，这里这么做的原因是因为，模型的输入是one-hot向量，需要把user和item都限制在Embedding的长度之内，\n",
    "        模型的两个输入的长度分别是user和item的数量，所以要重新从0编号。\n",
    "        \"\"\"\n",
    "        # 1. 新建名为\"userId\"的列，这列对用户从0开始编号\n",
    "        user_id = self._originalRatings[['uid']].drop_duplicates().reindex()\n",
    "        user_id['userId'] = np.arange(len(user_id)) #根据user的长度创建一个数组\n",
    "        # 将原先的DataFrame与user_id按照\"uid\"这一列进行合并\n",
    "        self._originalRatings = pd.merge(self._originalRatings, user_id, on=['uid'], how='left')\n",
    "\n",
    "        # 2. 对物品进行重新排列\n",
    "        item_id = self._originalRatings[['mid']].drop_duplicates()\n",
    "        item_id['itemId'] = np.arange(len(item_id))\n",
    "        self._originalRatings = pd.merge(self._originalRatings, item_id, on=['mid'], how='left')\n",
    "\n",
    "        # 按照['userId', 'itemId', 'rating', 'timestamp']的顺序重新排列\n",
    "        self._originalRatings = self._originalRatings[['userId', 'itemId', 'rating', 'timestamp','sust']]\n",
    "        print(self._originalRatings)\n",
    "        print('Range of userId is [{}, {}]'.format(self._originalRatings.userId.min(), self._originalRatings.userId.max()))\n",
    "        print('Range of itemId is [{}, {}]'.format(self._originalRatings.itemId.min(), self._originalRatings.itemId.max()))\n",
    "\n",
    "    def _binarize(self, ratings):\n",
    "        \"\"\"\n",
    "        binarize data into 0 or 1 for implicit feedback\n",
    "        \"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
    "        self._preprocessRatings = ratings\n",
    "        # print(\"binary: \\n\", self._preprocessRatings)\n",
    "\n",
    "    def _select_Negatives(self, ratings):\n",
    "        \"\"\"\n",
    "        Select al;l negative samples and 100 sampled negative items for each user.\n",
    "        \"\"\"\n",
    "        # 构造user-item表\n",
    "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
    "            columns={'itemId': 'interacted_items'})\n",
    "        print(\"interact_status: \\n\", interact_status)\n",
    "\n",
    "        # 把与用户没有产生过交互的样本都当做是负样本\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self._itemPool - x)\n",
    "\n",
    "        # 从上面的全部负样本中随机选99个出来\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
    "        print(\"after sampling interact_status: \\n\", interact_status)\n",
    "\n",
    "        print(\"select and rearrange columns\")\n",
    "        self._negatives = interact_status[['userId', 'negative_items', 'negative_samples']]\n",
    "\n",
    "    def _split_pool(self, ratings):\n",
    "        \"\"\"leave one out train/test split \"\"\"\n",
    "        print(\"sort by timestamp descend\")\n",
    "        # 先按照'userID'进行分组，然后根据时间戳降序排列\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        print(ratings)\n",
    "\n",
    "        # 选取排名第一的数据作为测试集，也就是最新的那个数据\n",
    "        test = ratings[ratings['rank_latest'] == 1]\n",
    "        # 选取所有排名靠后的，也就是历史数据当做训练集\n",
    "        train = ratings[ratings['rank_latest'] > 1]\n",
    "        # print(\"test: \\n\", test)\n",
    "        # print(\"train: \\n\", train)\n",
    "\n",
    "        print(\"size of test {0}, size of train {1}\".format(len(test), len(train)))\n",
    "\n",
    "        # 确保训练集和测试集的userId是一样的\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "\n",
    "        self.train_ratings = train[['userId', 'itemId', 'rating']]\n",
    "        self.test_ratings = test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def sample_generator(self, num_negatives):\n",
    "        # 合并之后的train_ratings的列包括['userId','itemId'，'rating','negative_items']\n",
    "        train_ratings = pd.merge(self.train_ratings, self._negatives[['userId', 'negative_items']], on='userId')\n",
    "        # 从用户的全部负样本集合中随机选择num_negatives个样本当做负样本，并产生一个新的名为\"negatives\"的列\n",
    "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "        print(train_ratings)\n",
    "\n",
    "        # 构造模型所需要的数据，分别是输入user、items以及目标分值ratings。\n",
    "        users, items, ratings = [], [], []\n",
    "        for row in train_ratings.itertuples():\n",
    "            # 构造正样本，分别是userId， itemId以及目标分值1\n",
    "            users.append(int(row.userId))\n",
    "            items.append(int(row.itemId))\n",
    "            ratings.append(float(row.rating))\n",
    "            # 为每个用户构造num_negatives个负样本，分别是userId， itemId以及目标分值0\n",
    "            for i in range(num_negatives):\n",
    "                users.append(int(row.userId))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0)) # 负样本的ratings为0，直接强行设置为0\n",
    "\n",
    "        return users, items, ratings\n",
    "    \n",
    "    def test_neg_generator(self, num_negatives):\n",
    "        # 合并之后的test_ratings的列包括['userId','itemId'，'rating','negative_items']\n",
    "        test_ratings = pd.merge(self.test_ratings, self._negatives[['userId', 'negative_items']], on='userId')\n",
    "        # 从用户的全部负样本集合中随机选择num_negatives个样本当做负样本，并产生一个新的名为\"negatives\"的列\n",
    "        test_ratings['negatives'] = test_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "        print(test_ratings)\n",
    "\n",
    "        # 构造模型所需要的数据，分别是输入user、items以及目标分值ratings。\n",
    "        users, items, ratings = [], [], []\n",
    "        for row in test_ratings.itertuples():\n",
    "            # 构造正样本，分别是userId， itemId以及目标分值1\n",
    "            users.append(int(row.userId))\n",
    "            items.append(int(row.itemId))\n",
    "            ratings.append(float(row.rating))\n",
    "            # 为每个用户构造num_negatives个负样本，分别是userId， itemId以及目标分值0\n",
    "            for i in range(num_negatives):\n",
    "                users.append(int(row.userId))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0)) # 负样本的ratings为0，直接强行设置为0\n",
    "\n",
    "        return test_ratings\n",
    "\n",
    "mag = DataProcess('duplicated_sample.csv')\n",
    "\n",
    "# generating files for test and train ratings\n",
    "train_ratings = mag.train_ratings\n",
    "train_ratings.to_csv('ml-1m.train.rating.csv',header=None, index=None)\n",
    "test_ratings = mag.test_ratings\n",
    "test_ratings.to_csv('ml-1m.test.rating.csv',header=None, index=None)\n",
    "\n",
    "# generating file for test negative\n",
    "ndf = mag.test_neg_generator(99)\n",
    "ndf['uipair'] = list(zip(ndf.userId, ndf.itemId))\n",
    "\n",
    "ndf2 = ndf.loc[:, ['uipair','negatives']]\n",
    "ndf2.to_csv('ml-1m.test.negative.csv',sep='\\t', header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4903d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating file for sustainable items\n",
    "susRatings = mag._originalRatings[mag._originalRatings['sust']==1]\n",
    "susRatings['itemId'].to_csv('sust.csv',header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35285a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n"
     ]
    }
   ],
   "source": [
    "print(len(susRatings['itemId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4cf55",
   "metadata": {},
   "source": [
    "### (trial) filter user based on duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "sampled1 = pd.read_csv('sampledData.csv',names=['uid','mid','rating','timestamp','sust'])\n",
    "sampled1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18495536",
   "metadata": {},
   "outputs": [],
   "source": [
    "noLabel2 = sampled1[sampled1['sust']==0]\n",
    "is_multi = noLabel2['uid'].value_counts()>5\n",
    "noLabel3 = noLabel2[noLabel2['uid'].isin(is_multi[is_multi].index)]\n",
    "noLabel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(noLabel3['uid'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ff5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "withLabel = sampled1[sampled1['sust']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([noLabel3,withLabel])\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf87554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(finalDf['uid'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmazonRecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aec2b49f0b156ccbd13eb3ced1f831c079eaadace1c55dfffcf3bc16a9a14954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
